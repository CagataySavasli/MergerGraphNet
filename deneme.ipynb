{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T08:42:54.586745Z",
     "start_time": "2025-06-22T08:42:52.317496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "# 1) Uyarıları kapatmak için\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "import os\n",
    "from lib.database.database_connector import DatabaseConnector\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertModel, AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm.auto import tqdm"
   ],
   "id": "391f7523af0070bc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cagatay/.cache/pypoetry/virtualenvs/mergergraphnet-5CuCFj11-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T08:42:54.655422Z",
     "start_time": "2025-06-22T08:42:54.653184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Ayarlar\n",
    "MODEL_NAME    = \"google/bert_uncased_L-2_H-128_A-2\"  # Çok küçük BERT\n",
    "MAX_LEN       = 512\n",
    "BATCH_SIZE    = 1\n",
    "NUM_EPOCHS    = 3\n",
    "DEVICE        = torch.device(\"cpu\")#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "be99084a3aa9f4a8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T08:42:56.105574Z",
     "start_time": "2025-06-22T08:42:54.703647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2) Tokenizer ve Tiny BERT\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "tinybert  = BertModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "tinybert.gradient_checkpointing_enable()  # Bellek tasarrufu"
   ],
   "id": "8ad3e7eb78567822",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T08:42:56.116277Z",
     "start_time": "2025-06-22T08:42:56.114471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in tinybert.named_parameters():\n",
    "    print(name)"
   ],
   "id": "5730fe682b0ab8f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T08:42:56.645402Z",
     "start_time": "2025-06-22T08:42:56.174689Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # 3) Veri tabanından çek\n",
    "db          = DatabaseConnector(\"./data/database.db\")\n",
    "cursor      = db.cursor\n",
    "train_data  = cursor.execute(\n",
    "    \"SELECT sentences, label FROM embeddings WHERE filing_date < '2020-01-01';\"\n",
    ").fetchall()\n",
    "test_data   = cursor.execute(\n",
    "    \"SELECT sentences, label FROM embeddings WHERE filing_date >= '2020-01-01';\"\n",
    ").fetchall()"
   ],
   "id": "e054fb5a03f3426a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T08:42:56.658966Z",
     "start_time": "2025-06-22T08:42:56.656209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4) Dataset: rapor başına tüm chunk’lar\n",
    "class LongTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512):\n",
    "        self.samples = []\n",
    "        for sents, label in tqdm(data, desc=\"Veri hazırlanıyor\", unit=\"örnek\"):\n",
    "            text = \" \".join(sents)\n",
    "            enc  = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_overflowing_tokens=True,\n",
    "                padding=False\n",
    "            )\n",
    "            self.samples.append({\n",
    "                \"input_ids\": enc.input_ids,\n",
    "                \"attention_mask\": enc.attention_mask,\n",
    "                \"label\": label\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ],
   "id": "ae07d7215be9f80a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T08:42:56.719611Z",
     "start_time": "2025-06-22T08:42:56.714746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5) collate_fn\n",
    "def collate_fn(batch):\n",
    "    B = len(batch)\n",
    "    C = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    S = MAX_LEN\n",
    "\n",
    "    input_ids      = torch.full((B, C, S), tokenizer.pad_token_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((B, C, S), dtype=torch.long)\n",
    "    labels         = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "\n",
    "    for i, sample in enumerate(batch):\n",
    "        for j, (ids, mask) in enumerate(zip(sample[\"input_ids\"], sample[\"attention_mask\"])):\n",
    "            L = len(ids)\n",
    "            input_ids[i, j, :L]      = torch.tensor(ids)\n",
    "            attention_mask[i, j, :L] = torch.tensor(mask)\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ],
   "id": "3f11ecd9e42a7d98",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T08:55:22.823006Z",
     "start_time": "2025-06-22T08:42:56.769849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 6) DataLoader\n",
    "train_ds      = LongTextDataset(train_data, tokenizer, MAX_LEN)\n",
    "test_ds       = LongTextDataset(test_data,  tokenizer, MAX_LEN)\n",
    "train_loader  = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           collate_fn=collate_fn, num_workers=0, pin_memory=True)\n",
    "test_loader   = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                           collate_fn=collate_fn, num_workers=0, pin_memory=True)"
   ],
   "id": "423ca8eaa4ef2a5b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Veri hazırlanıyor: 100%|██████████| 7878/7878 [10:49<00:00, 12.13örnek/s]\n",
      "Veri hazırlanıyor: 100%|██████████| 1027/1027 [01:36<00:00, 10.65örnek/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T08:55:22.845328Z",
     "start_time": "2025-06-22T08:55:22.839918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7) Model: Hierarchical DistilBERT + LSTM + FC\n",
    "class HierarchicalDistilBERT(nn.Module):\n",
    "    def __init__(self, distilbert, lstm_hidden=128, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.distilbert = distilbert\n",
    "        self.lstm       = nn.LSTM(\n",
    "            input_size=distilbert.config.hidden_size,\n",
    "            hidden_size=lstm_hidden,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.classifier = nn.Linear(2 * lstm_hidden, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        B, C, S = input_ids.size()\n",
    "        flat_ids  = input_ids.view(B*C, S)\n",
    "        flat_mask = attention_mask.view(B*C, S)\n",
    "\n",
    "        outputs = self.distilbert(input_ids=flat_ids, attention_mask=flat_mask)\n",
    "        pooled  = outputs.last_hidden_state[:, 0, :]    # [CLS] token embedding\n",
    "\n",
    "        chunk_embs = pooled.view(B, C, -1)              # [B, C, H]\n",
    "        lstm_out, _ = self.lstm(chunk_embs)             # [B, C, 2*H_lstm]\n",
    "        doc_emb     = lstm_out[:, -1, :]                # son zaman adımı\n",
    "\n",
    "        return self.classifier(doc_emb)                 # [B, num_labels]"
   ],
   "id": "ad2ab547e981e9d6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T08:55:22.917585Z",
     "start_time": "2025-06-22T08:55:22.897646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model     = HierarchicalDistilBERT(tinybert).to(DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "id": "6727094e6882869a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cagatay/.cache/pypoetry/virtualenvs/mergergraphnet-5CuCFj11-py3.12/lib/python3.12/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T09:59:10.260203Z",
     "start_time": "2025-06-22T08:55:22.953755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8) Eğitim Döngüsü\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", unit=\"batch\")\n",
    "    for batch in loop:\n",
    "        ids    = batch[\"input_ids\"].to(DEVICE)\n",
    "        mask   = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(ids, mask)\n",
    "        loss   = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_postfix(loss=f\"{loss.item():.4f}\")\n"
   ],
   "id": "fdf007ee7aa0c435",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 7878/7878 [9:08:00<00:00,  4.17s/batch, loss=1.6031]   \n",
      "Epoch 2/3: 100%|██████████| 7878/7878 [8:21:09<00:00,  3.82s/batch, loss=0.1846]   \n",
      "Epoch 3/3: 100%|██████████| 7878/7878 [7:34:37<00:00,  3.46s/batch, loss=0.1629]   \n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T10:04:46.770168Z",
     "start_time": "2025-06-23T09:59:10.328274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 9) Test ve Metrikler\n",
    "model.eval()\n",
    "all_labels, all_preds = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Test\", unit=\"batch\"):\n",
    "        ids    = batch[\"input_ids\"].to(DEVICE)\n",
    "        mask   = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        logits = model(ids, mask)\n",
    "        preds  = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())"
   ],
   "id": "43769b1b6f40379f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 1027/1027 [05:36<00:00,  3.05batch/s]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T10:04:46.815837Z",
     "start_time": "2025-06-23T10:04:46.795894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "acc, prec, rec, f1 = (\n",
    "    accuracy_score(all_labels, all_preds),\n",
    "    precision_score(all_labels, all_preds),\n",
    "    recall_score(all_labels, all_preds),\n",
    "    f1_score(all_labels, all_preds),\n",
    ")\n",
    "tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()"
   ],
   "id": "e18e37edd08583b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cagatay/.cache/pypoetry/virtualenvs/mergergraphnet-5CuCFj11-py3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T10:04:46.856533Z",
     "start_time": "2025-06-23T10:04:46.853579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Accuracy:  {acc:.3f}\")\n",
    "print(f\"Precision: {prec:.3f}\")\n",
    "print(f\"Recall:    {rec:.3f}\")\n",
    "print(f\"F1:        {f1:.3f}\")\n",
    "print(f\"TP:        {tp}\")\n",
    "print(f\"TN:        {tn}\")\n",
    "print(f\"FP:        {fp}\")\n",
    "print(f\"FN:        {fn}\")"
   ],
   "id": "5edfa87ee8d956d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.853\n",
      "Precision: 0.000\n",
      "Recall:    0.000\n",
      "F1:        0.000\n",
      "TP:        0\n",
      "TN:        876\n",
      "FP:        0\n",
      "FN:        151\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T10:04:46.908278Z",
     "start_time": "2025-06-23T10:04:46.906992Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ac39571cda05baa3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
